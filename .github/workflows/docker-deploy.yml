name: CI/CD ComplÃ¨te

on:
  push:
    branches:
      - main

env:
  # Configuration K3s
  K3S_VERSION: v1.26.5+k3s1
  K8S_FRONTEND_PATH: ./k8s
  NODE_PORT: 30080
  CANARY_PORT: 30083
  
  # Configuration Docker
  DOCKER_IMAGE: ${{ secrets.DOCKER_USERNAME }}/front_end
  CONTAINER_NAME: frontend-prod
  PORT: 80
  DOCKER_BUILDKIT: 1
  
  # Configuration SonarQube
  SONARQUBE_CONTAINER_NAME: sonarqube
  SONARQUBE_PORT: 9000
  SONARQUBE_VERSION: 10.4.1-community
  
  # Configuration Portainer
  PORTAINER_USER: admin
  PORTAINER_PASS: ${{ secrets.PORTAINER_PASSWORD }}
  PORTAINER_PORT: 19000
  PORTAINER_HTTPS_PORT: 19443

  # Configuration Monitoring
  GRAFANA_PORT: 30091
  PROMETHEUS_PORT: 30090
  ALERTMANAGER_PORT: 30092

  # alertmanager
  SMTP_USER: ${{ secrets.SMTP_USER }}
  SMTP_PASSWORD: ${{ secrets.SMTP_PASSWORD }}

jobs:
  build-app:
    runs-on: self-hosted
    timeout-minutes: 30
    steps:
      - name: ðŸ›Ž Checkout du code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: âŽ” Configurer Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 18
          cache: 'npm'

      - name: ðŸ“¦ Installer les dÃ©pendances
        run: |
          npm ci --prefer-offline
          npm install -D @vitest/coverage-v8

      - name: ðŸ› ï¸ Builder l'application
        run: |
          npm run build
          npm test -- --coverage --coverage.provider=v8 --coverage.reporter=lcov
          echo "=== VÃ©rification des rapports de couverture ==="
          ls -la coverage/
          [ -f "coverage/lcov.info" ] || exit 1
          echo "=== Build gÃ©nÃ©rÃ© ==="
          ls -lh dist/

      - name: ðŸ“¦ Sauvegarder les artefacts
        uses: actions/upload-artifact@v4
        with:
          name: build-artifacts
          path: |
            dist/
            coverage/
          retention-days: 1

  unit-tests:
    needs: build-app
    runs-on: self-hosted
    timeout-minutes: 20
    steps:
      - name: ðŸ›Ž Checkout du code
        uses: actions/checkout@v3

      - name: âŽ” Configurer Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 18
          cache: 'npm'

      - name: ðŸ“¦ Installer les dÃ©pendances
        run: npm ci --prefer-offline

      - name: ðŸ§ª ExÃ©cuter les tests unitaires
        run: npm test
        env:
          CI: true
          NODE_ENV: test

  sonarqube-analysis:
    needs: unit-tests
    runs-on: self-hosted
    timeout-minutes: 20
    steps:
      - name: ðŸ›Žï¸ Checkout du code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: ðŸ” VÃ©rifier si SonarQube est dÃ©jÃ  en cours d'exÃ©cution
        id: check-sonarqube
        run: |
          if docker ps -q --filter "name=${{ env.SONARQUBE_CONTAINER_NAME }}"; then
            echo "sonarqube_running=true" >> $GITHUB_OUTPUT
            echo "ðŸ”„ SonarQube est dÃ©jÃ  en cours d'exÃ©cution"
          else
            echo "sonarqube_running=false" >> $GITHUB_OUTPUT
            echo "âš ï¸ SonarQube n'est pas dÃ©marrÃ©, dÃ©marrage..."
            docker run -d \
              --name ${{ env.SONARQUBE_CONTAINER_NAME }} \
              -p ${{ env.SONARQUBE_PORT }}:9000 \
              -v sonarqube_data:/opt/sonarqube/data \
              -v sonarqube_extensions:/opt/sonarqube/extensions \
              -v sonarqube_logs:/opt/sonarqube/logs \
              -e SONAR_ES_BOOTSTRAP_CHECKS_DISABLE=true \
              sonarqube:${{ env.SONARQUBE_VERSION }}
          fi

      - name: â³ Attendre que SonarQube soit prÃªt
        run: |
          for i in {1..30}; do
            if curl -sSf "${{ secrets.SONAR_HOST_URL }}/api/system/status" | grep -q '"status":"UP"'; then
              echo "âœ… SonarQube est opÃ©rationnel"
              exit 0
            fi
            sleep 5
            echo "âŒ› Tentative $i/30 - En attente..."
          done
          echo "âŒ SonarQube n'a pas dÃ©marrÃ© dans le dÃ©lai imparti"
          exit 1

      - name: ðŸ” ExÃ©cuter l'analyse SonarQube
        uses: SonarSource/sonarqube-scan-action@v4
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}
        with:
          args: >
            -Dsonar.projectKey=Frontend_project
            -Dsonar.projectName="Frontend_project"
            -Dsonar.sources=src
            -Dsonar.javascript.lcov.reportPaths=coverage/lcov.info
            -Dsonar.exclusions=**/node_modules/**,**/*.test.js
            -Dsonar.sourceEncoding=UTF-8
            -Dsonar.qualitygate.wait=true
            -Dsonar.login=${{ secrets.SONAR_TOKEN }}

      - name: ðŸ“Œ Afficher les rÃ©sultats
        run: |
          echo "=========================================="
          echo "ðŸ” Analyse SonarQube terminÃ©e avec succÃ¨s !"
          echo "ðŸŒ AccÃ¨s : ${{ secrets.SONAR_HOST_URL }}/dashboard?id=Frontend_project"
          echo "=========================================="

  build-and-push:
    needs: sonarqube-analysis
    runs-on: self-hosted
    timeout-minutes: 30
    steps:
      - name: ðŸ”‘ Authentification Docker Hub
        run: echo "${{ secrets.DOCKER_TOKEN }}" | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin

      - name: ðŸ“¥ RÃ©cupÃ©rer l'artefact
        uses: actions/download-artifact@v4
        with:
          name: build-artifacts
          path: dist/

      - name: ðŸ³ Build et Push Docker
        run: |
          docker build \
            -t $DOCKER_IMAGE:latest \
            -t $DOCKER_IMAGE:$GITHUB_SHA \
            .
          
          docker push $DOCKER_IMAGE:latest
          docker push $DOCKER_IMAGE:$GITHUB_SHA

  deploy-docker:
    needs: build-and-push
    runs-on: self-hosted
    timeout-minutes: 20
    steps:
      - name: ðŸ”‘ Authentification Docker
        run: echo "${{ secrets.DOCKER_TOKEN }}" | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin

      - name: ï¿½ Nettoyage
        run: |
          docker stop $CONTAINER_NAME || true
          docker rm -f $CONTAINER_NAME || true

      - name: ðŸš€ DÃ©ploiement
        run: |
          docker pull $DOCKER_IMAGE:latest
          docker run -d \
            --name $CONTAINER_NAME \
            -p $PORT:80 \
            --restart unless-stopped \
            -e NODE_ENV=production \
            $DOCKER_IMAGE:latest

      - name: âœ… VÃ©rification
        run: |
          sleep 15
          curl -sSf http://localhost:$PORT/ || exit 1
          echo "DÃ©ploiement Docker rÃ©ussi!"

  deploy-k3s:
    needs: deploy-docker
    runs-on: self-hosted
    timeout-minutes: 40
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      
      - name: ðŸ› ï¸ Configuration K3s
        run: |
          set -x
          sudo k3s-uninstall.sh || true
          sudo rm -rf /etc/rancher/k3s /var/lib/rancher/k3s /var/lib/cni /etc/cni/net.d/*
          
          curl -sfL https://get.k3s.io | \
            INSTALL_K3S_VERSION=$K3S_VERSION \
            sh -s - \
            --write-kubeconfig-mode 644 \
            --disable traefik \
            --node-ip $(hostname -I | awk '{print $1}') \
            --docker

      - name: ðŸ”§ Configuration kubectl
        run: |
          mkdir -p ~/.kube
          sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
          sudo chown $USER:$USER ~/.kube/config
          sudo chmod 600 ~/.kube/config
          sed -i "s/127.0.0.1/$(hostname -I | awk '{print $1}')/g" ~/.kube/config
          
          kubectl cluster-info
          kubectl get nodes -o wide

      - name: ðŸ“Œ Sauvegarde rÃ©vision actuelle
        id: save-revision
        run: |
          CURRENT_REVISION=$(kubectl get deployment frontend-deployment -n frontend -o jsonpath='{.metadata.annotations.deployment\.kubernetes\.io/revision}' 2>/dev/null || echo "0")
          echo "CURRENT_REVISION=$CURRENT_REVISION" >> $GITHUB_ENV
          echo "RÃ©vision actuelle sauvegardÃ©e: $CURRENT_REVISION"

      - name: ðŸš€ DÃ©ploiement K8s (Production)
        run: |
          kubectl apply -f $K8S_FRONTEND_PATH/namespace.yaml
          
          export DOCKER_IMAGE=$DOCKER_IMAGE
          export TAG=$GITHUB_SHA
          
          envsubst '${DOCKER_IMAGE} ${TAG}' < $K8S_FRONTEND_PATH/deployment.yaml | kubectl apply -f -
          kubectl apply -f $K8S_FRONTEND_PATH/service.yaml

      - name: ðŸš€ DÃ©ploiement Canary
        run: |
          export DOCKER_IMAGE=$DOCKER_IMAGE
          export TAG=$GITHUB_SHA
          envsubst '${DOCKER_IMAGE} ${TAG}' < $K8S_FRONTEND_PATH/canary-strategy-complete.yaml | kubectl apply -f -

      - name: ðŸ” VÃ©rification dÃ©ploiement
        id: verify-deployment
        run: |
          if kubectl wait --for=condition=available deployment/frontend-deployment -n frontend --timeout=300s; then
            echo "status=success" >> $GITHUB_OUTPUT
            kubectl get pods,svc -n frontend -o wide
            echo "Application Production disponible sur: http://$(hostname -I | awk '{print $1}'):$NODE_PORT"
            echo "Application Canary disponible sur: http://$(hostname -I | awk '{print $1}'):$CANARY_PORT"
          else
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "::error::Ã‰chec de la vÃ©rification du dÃ©ploiement"
          fi

      - name: ðŸ”„ Rollback si Ã©chec
        if: steps.verify-deployment.outputs.status == 'failure'
        run: |
          echo "::error::Lancement du rollback Ã  la rÃ©vision ${{ env.CURRENT_REVISION }}"
          kubectl rollout undo deployment/frontend-deployment -n frontend --to-revision=${{ env.CURRENT_REVISION }}
          kubectl delete -f $K8S_FRONTEND_PATH/canary-strategy-complete.yaml --ignore-not-found
          
          echo "VÃ©rification du rollback..."
          kubectl rollout status deployment/frontend-deployment -n frontend --timeout=180s
          
          echo "Ã‰tat aprÃ¨s rollback:"
          kubectl get pods -n frontend
          exit 1

      - name: ðŸ•µï¸ VÃ©rification Canary
        if: steps.verify-deployment.outputs.status == 'success'
        run: |
          echo "=== VÃ©rification du dÃ©ploiement Canary ==="
          kubectl wait --for=condition=available deployment/frontend-canary -n frontend --timeout=120s
          
          echo "=== Test d'accÃ¨s Canary ==="
          CANARY_URL="http://$(hostname -I | awk '{print $1}'):$CANARY_PORT"
          if curl -sSf $CANARY_URL >/dev/null; then
            echo "Canary est accessible avec succÃ¨s: $CANARY_URL"
          else
            echo "::error::Ã‰chec d'accÃ¨s au Canary"
            exit 1
          fi

      - name: ðŸ³ DÃ©ploiement Portainer
        if: steps.verify-deployment.outputs.status == 'success'
        run: |
          docker stop portainer || true
          docker rm -f portainer || true
          
          docker run -d \
            --name portainer \
            -p $PORTAINER_PORT:9000 \
            -p $PORTAINER_HTTPS_PORT:9443 \
            -v /var/run/docker.sock:/var/run/docker.sock \
            -v portainer_data:/data \
            --restart unless-stopped \
            portainer/portainer-ce:latest
          
          sleep 30
          
          if ! curl -sSf http://localhost:$PORTAINER_PORT/api/users/admin/check >/dev/null; then
            curl -X POST "http://localhost:$PORTAINER_PORT/api/users/admin/init" \
              -H "Content-Type: application/json" \
              -d '{"Username": "'"$PORTAINER_USER"'", "Password": "'"$PORTAINER_PASS"'"}'
          fi
          
          AUTH_TOKEN=$(curl -s -X POST "http://localhost:$PORTAINER_PORT/api/auth" \
            -d '{"Username": "'"$PORTAINER_USER"'", "Password": "'"$PORTAINER_PASS"'"}' \
            | jq -r '.jwt')
          
          K3S_IP=$(hostname -I | awk '{print $1}')
          K3S_ENDPOINT="https://$K3S_IP:6443"
          
          K3S_CONFIG=$(sudo cat /etc/rancher/k3s/k3s.yaml | \
            sed "s/127.0.0.1/$K3S_IP/g" | \
            sed "s/default/portainer-k3s/g" | \
            base64 -w0)
          
          curl -X POST "http://localhost:$PORTAINER_PORT/api/endpoints" \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer $AUTH_TOKEN" \
            -d '{
              "Name": "K3s Cluster",
              "URL": "'"$K3S_ENDPOINT"'",
              "TLS": true,
              "TLSSkipVerify": true,
              "Type": 3,
              "KubernetesConfig": {
                "Configuration": "'"$K3S_CONFIG"'",
                "UseLoadBalancer": false
              }
            }'
            
          echo "Portainer configurÃ© avec accÃ¨s Ã  K3s: http://$K3S_IP:$PORTAINER_PORT"

  cluster-monitoring:
    needs: deploy-k3s
    runs-on: self-hosted
    timeout-minutes: 20
    steps:
      - name: ðŸ”§ Configurer kubectl
        run: |
          mkdir -p ~/.kube
          sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
          sudo chown $USER:$USER ~/.kube/config
          sed -i "s/127.0.0.1/$(hostname -I | awk '{print $1}')/g" ~/.kube/config

      - name: ðŸ“¥ TÃ©lÃ©charger les fichiers de configuration
        uses: actions/checkout@v4
        with:
          path: monitoring-config

      - name: ðŸ› ï¸ DÃ©ployer AlertManager (sans Helm)
        run: |
          # CrÃ©er le namespace si inexistant
          kubectl create namespace monitoring || true

          # CrÃ©er la configuration AlertManager avec interpolation des secrets
          sed \
            -e "s|\${{ secrets.SMTP_USER }}|${{ secrets.SMTP_USER }}|g" \
            -e "s|\${{ secrets.SMTP_PASSWORD }}|${{ secrets.SMTP_PASSWORD }}|g" \
            monitoring-config/monitoring-config/alertmanager-config.yml > alertmanager-resolved.yml

          # CrÃ©er le secret de configuration
          kubectl -n monitoring create secret generic alertmanager-config \
            --from-file=alertmanager.yml=alertmanager-resolved.yml \
            --dry-run=client -o yaml | kubectl apply -f -

          # DÃ©ployer AlertManager avec un simple Deployment
          kubectl apply -f - <<EOF
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: alertmanager
            namespace: monitoring
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: alertmanager
            template:
              metadata:
                labels:
                  app: alertmanager
              spec:
                containers:
                - name: alertmanager
                  image: quay.io/prometheus/alertmanager:v0.25.0
                  args:
                  - '--config.file=/etc/alertmanager/alertmanager.yml'
                  - '--storage.path=/alertmanager'
                  ports:
                  - containerPort: 9093
                    name: http
                  volumeMounts:
                  - name: config
                    mountPath: /etc/alertmanager
                volumes:
                - name: config
                  secret:
                    secretName: alertmanager-config
          EOF

          # CrÃ©er le Service NodePort
          kubectl apply -f - <<EOF
          apiVersion: v1
          kind: Service
          metadata:
            name: alertmanager
            namespace: monitoring
          spec:
            type: NodePort
            ports:
            - port: 9093
              targetPort: http
              nodePort: $ALERTMANAGER_PORT
            selector:
              app: alertmanager
          EOF

      - name: âŽˆ Installer Prometheus+Grafana avec Helm
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm upgrade --install monitoring prometheus-community/kube-prometheus-stack \
            --namespace monitoring \
            --set grafana.adminUser="${{ secrets.GRAFANA_ADMIN_USER }}" \
            --set grafana.adminPassword="${{ secrets.GRAFANA_ADMIN_PASSWORD }}" \
            --set prometheus.service.type=NodePort \
            --set prometheus.service.nodePort=$PROMETHEUS_PORT \
            --set grafana.service.type=NodePort \
            --set grafana.service.nodePort=$GRAFANA_PORT \
            --set alertmanager.enabled=false \
            --set prometheus.prometheusSpec.ruleSelectorNilUsesHelmValues=false \
            --set prometheus.prometheusSpec.ruleNamespaceSelector.matchLabels.name="monitoring"

      - name: ðŸš¨ DÃ©ployer les alertes du cluster
        run: |
          # VÃ©rifier que le fichier d'alertes existe
          if [ ! -f "monitoring-config/cluster-alerts.yml" ]; then
            echo "âŒ Fichier cluster-alerts.yml manquant dans monitoring-config/"
            exit 1
          fi

          # Appliquer les rÃ¨gles d'alerte
          kubectl apply -f monitoring-config/cluster-alerts.yml -n monitoring

          # VÃ©rifier le dÃ©ploiement des rÃ¨gles
          echo "=== RÃ¨gles dÃ©ployÃ©es ==="
          kubectl get prometheusrules -n monitoring -o wide
          echo "=== DÃ©tails des rÃ¨gles ==="
          kubectl describe prometheusrules -n monitoring

      - name: ðŸ” VÃ©rifier la configuration
        run: |
          # VÃ©rifier la configuration AlertManager
          echo "=== Configuration AlertManager ==="
          kubectl -n monitoring exec alertmanager-monitoring-kube-prometheus-alertmanager-0 -- \
            cat /etc/alertmanager/alertmanager.yml

          # VÃ©rifier les alertes actives dans Prometheus
          echo "=== Alertes dans Prometheus ==="
          PROM_POD=$(kubectl get pods -n monitoring -l app.kubernetes.io/name=prometheus -o jsonpath='{.items[0].metadata.name}')
          kubectl -n monitoring exec $PROM_POD -- \
            curl -s http://localhost:9090/api/v1/alerts | jq '.data.alerts[] | .labels.alertname'

      - name: ðŸ“© Tester les notifications
        run: |
          # Envoyer une alerte de test
          echo "=== Envoi d'une alerte test ==="
          kubectl -n monitoring exec alertmanager-monitoring-kube-prometheus-alertmanager-0 -- \
            amtool alert add \
            alertname="IntegrationTestAlert" \
            severity="critical" \
            --annotation=summary="Test d'intÃ©gration" \
            --annotation=description="Ceci est un test de notification par email"

          # VÃ©rifier que l'alerte a Ã©tÃ© traitÃ©e
          sleep 10
          echo "=== DerniÃ¨res alertes ==="
          kubectl -n monitoring exec alertmanager-monitoring-kube-prometheus-alertmanager-0 -- \
            amtool alert query

      - name: ðŸ” VÃ©rifier les dÃ©ploiements
        run: |
          echo "=== Pods AlertManager ==="
          kubectl -n monitoring get pods -l app=alertmanager
          echo "=== Logs AlertManager ==="
          kubectl -n monitoring logs -l app=alertmanager --tail=50
          echo "=== Services ==="
          kubectl -n monitoring get svc

      - name: ðŸŒ Afficher les accÃ¨s
        run: |
          CLUSTER_IP=$(hostname -I | awk '{print $1}')
          echo "================================================"
          echo "ðŸ”” NOTIFICATIONS ALERTMANAGER CONFIGURÃ‰ES ðŸ””"
          echo "================================================"
          echo ""
          echo "ðŸ“Š AccÃ¨s aux interfaces :"
          echo "- Prometheus    : http://$CLUSTER_IP:30090"
          echo "- AlertManager  : http://$CLUSTER_IP:$ALERTMANAGER_PORT"
          echo "- Grafana       : http://$CLUSTER_IP:$GRAFANA_PORT"
          echo ""
          echo "ðŸ”‘ Identifiants Grafana :"
          echo "- Utilisateur : ${{ secrets.GRAFANA_ADMIN_USER }}"
          echo "- Mot de passe : ${{ secrets.GRAFANA_ADMIN_PASSWORD }}"
          echo ""
          echo "ðŸ“§ Notifications envoyÃ©es Ã  : ${{ secrets.SMTP_USER }}"
          echo ""
          echo "â„¹ï¸ Pour vÃ©rifier les emails, consultez la boÃ®te de rÃ©ception de ${{ secrets.SMTP_USER }}"
          echo "================================================"