name: chaine CI/CD complete 

on:
  push:
    branches:
      - main

env:
  # Configuration K3s
  K3S_VERSION: v1.26.5+k3s1
  K8S_FRONTEND_PATH: ./k8s
  NODE_PORT: 30080
  CANARY_PORT: 30083
  
  # Configuration Docker
  DOCKER_IMAGE: ${{ secrets.DOCKER_USERNAME }}/front_end
  CONTAINER_NAME: frontend-prod
  PORT: 80
  DOCKER_BUILDKIT: 1
  
  # Configuration SonarQube
  SONARQUBE_CONTAINER_NAME: sonarqube
  SONARQUBE_PORT: 9000
  SONARQUBE_VERSION: 10.4.1-community
  
  # Configuration Portainer
  PORTAINER_USER: admin
  PORTAINER_PASS: ${{ secrets.PORTAINER_PASSWORD }}
  PORTAINER_PORT: 19000
  PORTAINER_HTTPS_PORT: 19443

  # Configuration Monitoring
  GRAFANA_PORT: 30091
  PROMETHEUS_PORT: 30090
  ALERTMANAGER_PORT: 30092
  GRAFANA_ADMIN_USER: ${{ secrets.GRAFANA_ADMIN_USER }}
  GRAFANA_ADMIN_PASSWORD: ${{ secrets.GRAFANA_ADMIN_PASSWORD }}

  # Alertmanager
  SMTP_USER: ${{ secrets.SMTP_USER }}
  SMTP_PASSWORD: ${{ secrets.SMTP_PASSWORD }}

jobs:
  build-app:
    runs-on: self-hosted
    timeout-minutes: 30
    steps:
      - name: üõé Checkout du code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: ‚éî Configurer Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 18
          cache: 'npm'

      - name: üì¶ Installer les d√©pendances
        run: |
          npm ci --prefer-offline
          npm install -D @vitest/coverage-v8
      - name: üõ†Ô∏è Builder l'application
        run: |
          npm run build
          npm test -- --coverage --coverage.provider=v8 --coverage.reporter=lcov
          echo "=== V√©rification des rapports de couverture ==="
          ls -la coverage/
          [ -f "coverage/lcov.info" ] || exit 1
          echo "=== Build g√©n√©r√© ==="
          ls -lh dist/
      - name: üì¶ Sauvegarder les artefacts
        uses: actions/upload-artifact@v4
        with:
          name: build-artifacts
          path: |
            dist/
            coverage/
          retention-days: 1

  unit-tests:
    needs: build-app
    runs-on: self-hosted
    timeout-minutes: 20
    steps:
      - name: üõé Checkout du code
        uses: actions/checkout@v3

      - name: ‚éî Configurer Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 18
          cache: 'npm'

      - name: üì¶ Installer les d√©pendances
        run: npm ci --prefer-offline

      - name: üß™ Ex√©cuter les tests unitaires
        run: npm test
        env:
          CI: true
          NODE_ENV: test

  sonarqube-analysis:
    needs: unit-tests
    runs-on: self-hosted
    timeout-minutes: 20
    steps:
      - name: üõéÔ∏è Checkout du code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: üîç V√©rifier si SonarQube est d√©j√† en cours d'ex√©cution
        id: check-sonarqube
        run: |
          if docker ps -q --filter "name=${{ env.SONARQUBE_CONTAINER_NAME }}"; then
            echo "sonarqube_running=true" >> $GITHUB_OUTPUT
            echo "üîÑ SonarQube est d√©j√† en cours d'ex√©cution"
          else
            echo "sonarqube_running=false" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è SonarQube n'est pas d√©marr√©, d√©marrage..."
            docker run -d \
              --name ${{ env.SONARQUBE_CONTAINER_NAME }} \
              -p ${{ env.SONARQUBE_PORT }}:9000 \
              -v sonarqube_data:/opt/sonarqube/data \
              -v sonarqube_extensions:/opt/sonarqube/extensions \
              -v sonarqube_logs:/opt/sonarqube/logs \
              -e SONAR_ES_BOOTSTRAP_CHECKS_DISABLE=true \
              sonarqube:${{ env.SONARQUBE_VERSION }}
          fi
      - name: ‚è≥ Attendre que SonarQube soit pr√™t
        run: |
          for i in {1..30}; do
            if curl -sSf "${{ secrets.SONAR_HOST_URL }}/api/system/status" | grep -q '"status":"UP"'; then
              echo "‚úÖ SonarQube est op√©rationnel"
              exit 0
            fi
            sleep 5
            echo "‚åõ Tentative $i/30 - En attente..."
          done
          echo "‚ùå SonarQube n'a pas d√©marr√© dans le d√©lai imparti"
          exit 1
      - name: üîç Ex√©cuter l'analyse SonarQube
        uses: SonarSource/sonarqube-scan-action@v4
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}
        with:
          args: >
            -Dsonar.projectKey=Frontend_project
            -Dsonar.projectName="Frontend_project"
            -Dsonar.sources=src
            -Dsonar.javascript.lcov.reportPaths=coverage/lcov.info
            -Dsonar.exclusions=**/node_modules/**,**/*.test.js
            -Dsonar.sourceEncoding=UTF-8
            -Dsonar.qualitygate.wait=true
            -Dsonar.login=${{ secrets.SONAR_TOKEN }}
      - name: üìå Afficher les r√©sultats
        run: |
          echo "=========================================="
          echo "üîç Analyse SonarQube termin√©e avec succ√®s !"
          echo "üåê Acc√®s : ${{ secrets.SONAR_HOST_URL }}/dashboard?id=Frontend_project"
          echo "=========================================="
  build-and-push:
    needs: sonarqube-analysis
    runs-on: self-hosted
    timeout-minutes: 30
    steps:
      - name: üîë Authentification Docker Hub
        run: echo "${{ secrets.DOCKER_TOKEN }}" | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin

      - name: üì• R√©cup√©rer l'artefact
        uses: actions/download-artifact@v4
        with:
          name: build-artifacts
          path: dist/

      - name: üê≥ Build et Push Docker
        run: |
          docker build \
            -t $DOCKER_IMAGE:latest \
            -t $DOCKER_IMAGE:$GITHUB_SHA \
            .
          
          docker push $DOCKER_IMAGE:latest
          docker push $DOCKER_IMAGE:$GITHUB_SHA
  deploy-docker:
    needs: build-and-push
    runs-on: self-hosted
    timeout-minutes: 20
    steps:
      - name: üîë Authentification Docker
        run: echo "${{ secrets.DOCKER_TOKEN }}" | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin

      - name: üßπ Nettoyage
        run: |
          docker stop $CONTAINER_NAME || true
          docker rm -f $CONTAINER_NAME || true
      - name: üöÄ D√©ploiement
        run: |
          docker pull $DOCKER_IMAGE:latest
          docker run -d \
            --name $CONTAINER_NAME \
            -p $PORT:80 \
            --restart unless-stopped \
            -e NODE_ENV=production \
            $DOCKER_IMAGE:latest
      - name: ‚úÖ V√©rification
        run: |
          sleep 15
          curl -sSf http://localhost:$PORT/ || exit 1
          echo "D√©ploiement Docker r√©ussi!"
  deploy-k3s:
    needs: deploy-docker
    runs-on: self-hosted
    timeout-minutes: 40
    steps:
      - name: Checkout
        uses: actions/checkout@v3
      
      -  name: Configure kubeconfig
         run: |
          mkdir -p ~/.kube
          echo "${{ secrets.KUBECONFIG}}" > ~/.kube/config
          chmod 600 ~/.kube/config
          kubectl get nodes   
      - name: Verify cluster access
        run: kubectl cluster-info

      - name: üöÄ D√©ploiement K8s (Production)
        run: |
          kubectl apply -f $K8S_FRONTEND_PATH/namespace.yaml
          
          export DOCKER_IMAGE=$DOCKER_IMAGE
          export TAG=$GITHUB_SHA
          
          envsubst '${DOCKER_IMAGE} ${TAG}' < $K8S_FRONTEND_PATH/deployment.yaml | kubectl apply -f -
          kubectl apply -f $K8S_FRONTEND_PATH/service.yaml
      - name: üöÄ D√©ploiement Canary
        run: |
          export DOCKER_IMAGE=$DOCKER_IMAGE
          export TAG=$GITHUB_SHA
          envsubst '${DOCKER_IMAGE} ${TAG}' < $K8S_FRONTEND_PATH/canary-strategy-complete.yaml | kubectl apply -f -
      - name: üîç V√©rification d√©ploiement
        id: verify-deployment
        run: |
          if kubectl wait --for=condition=available deployment/frontend-deployment -n frontend --timeout=300s; then
            echo "status=success" >> $GITHUB_OUTPUT
            kubectl get pods,svc -n frontend -o wide
            echo "Application Production disponible sur: http://$(hostname -I | awk '{print $1}'):$NODE_PORT"
            echo "Application Canary disponible sur: http://$(hostname -I | awk '{print $1}'):$CANARY_PORT"
          else
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "::error::√âchec de la v√©rification du d√©ploiement"
          fi
      - name: üîÑ Rollback si √©chec
        if: steps.verify-deployment.outputs.status == 'failure'
        run: |
          echo "::error::Lancement du rollback √† la r√©vision ${{ env.CURRENT_REVISION }}"
          kubectl rollout undo deployment/frontend-deployment -n frontend --to-revision=${{ env.CURRENT_REVISION }}
          kubectl delete -f $K8S_FRONTEND_PATH/canary-strategy-complete.yaml --ignore-not-found
          
          echo "V√©rification du rollback..."
          kubectl rollout status deployment/frontend-deployment -n frontend --timeout=180s
          
          echo "√âtat apr√®s rollback:"
          kubectl get pods -n frontend
          exit 1
      - name: üïµÔ∏è V√©rification Canary
        if: steps.verify-deployment.outputs.status == 'success'
        run: |
          echo "=== V√©rification du d√©ploiement Canary ==="
          kubectl wait --for=condition=available deployment/frontend-canary -n frontend --timeout=120s
          
          echo "=== Test d'acc√®s Canary ==="
          CANARY_URL="http://$(hostname -I | awk '{print $1}'):$CANARY_PORT"
          if curl -sSf $CANARY_URL >/dev/null; then
            echo "Canary est accessible avec succ√®s: $CANARY_URL"
          else
            echo "::error::√âchec d'acc√®s au Canary"
            exit 1
          fi
      - name: üê≥ D√©ploiement Portainer
        if: steps.verify-deployment.outputs.status == 'success'
        run: |
          docker stop portainer || true
          docker rm -f portainer || true
          
          docker run -d \
            --name portainer \
            -p $PORTAINER_PORT:9000 \
            -p $PORTAINER_HTTPS_PORT:9443 \
            -v /var/run/docker.sock:/var/run/docker.sock \
            -v portainer_data:/data \
            --restart unless-stopped \
            portainer/portainer-ce:latest
          
          sleep 30
          
          if ! curl -sSf http://localhost:$PORTAINER_PORT/api/users/admin/check >/dev/null; then
            curl -X POST "http://localhost:$PORTAINER_PORT/api/users/admin/init" \
              -H "Content-Type: application/json" \
              -d '{"Username": "'"$PORTAINER_USER"'", "Password": "'"$PORTAINER_PASS"'"}'
          fi
          
          AUTH_TOKEN=$(curl -s -X POST "http://localhost:$PORTAINER_PORT/api/auth" \
            -d '{"Username": "'"$PORTAINER_USER"'", "Password": "'"$PORTAINER_PASS"'"}' \
            | jq -r '.jwt')
          
          K3S_IP=$(hostname -I | awk '{print $1}')
          K3S_ENDPOINT="https://$K3S_IP:6443"
          
          K3S_CONFIG=$(sudo cat /etc/rancher/k3s/k3s.yaml | \
            sed "s/127.0.0.1/$K3S_IP/g" | \
            sed "s/default/portainer-k3s/g" | \
            base64 -w0)
          
          curl -X POST "http://localhost:$PORTAINER_PORT/api/endpoints" \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer $AUTH_TOKEN" \
            -d '{
              "Name": "K3s Cluster",
              "URL": "'"$K3S_ENDPOINT"'",
              "TLS": true,
              "TLSSkipVerify": true,
              "Type": 3,
              "KubernetesConfig": {
                "Configuration": "'"$K3S_CONFIG"'",
                "UseLoadBalancer": false
              }
            }'
            
          echo "Portainer configur√© avec acc√®s √† K3s: http://$K3S_IP:$PORTAINER_PORT"

  cluster-monitoring:
    needs: deploy-k3s
    runs-on: self-hosted
    timeout-minutes: 30


    steps:
      - name: üîß Configurer kubectl
        run: |
          mkdir -p ~/.kube
          echo "${{ secrets.KUBECONFIG }}" > ~/.kube/config
          chmod 600 ~/.kube/config
          kubectl cluster-info

      - name: üì• T√©l√©charger les fichiers de configuration
        uses: actions/checkout@v4
        with:
          path: '.'

      - name: üõ†Ô∏è V√©rifier l'existence des fichiers
        run: |
          echo "=== Contenu du dossier monitoring-config ==="
          ls -la monitoring-config/
          ls -la monitoring-config/alertmanager/
          ls -la monitoring-config/Prometheus/
          [ -f "monitoring-config/alertmanager/alertmanager-config.yml" ] || { echo "::error::Fichier alertmanager-config.yml manquant"; exit 1; }
          [ -f "monitoring-config/alertmanager/alertmanager-deployment.yml" ] || { echo "::error::Fichier alertmanager-deployment.yml manquant"; exit 1; }
          [ -f "monitoring-config/Prometheus/alert-rules.yml" ] || { echo "::error::Fichier alert-rules.yml manquant"; exit 1; }

      - name: üõ†Ô∏è D√©ployer AlertManager
        timeout-minutes: 20
        run: |
          # Appliquer la configuration
          kubectl apply -f monitoring-config/alertmanager/alertmanager-deployment.yml
          
          # Attendre avec gestion des erreurs am√©lior√©e
          for i in {1..5}; do
            if kubectl -n monitoring wait --for=condition=ready pod -l app=alertmanager --timeout=180s; then
              echo "‚úÖ AlertManager d√©marr√© avec succ√®s"
              exit 0
            fi
            
            echo "‚ö†Ô∏è Tentative $i/5 √©chou√©e. Diagnostic..."
            
            # Collecte des logs et informations de d√©bogage
            kubectl -n monitoring get pods -l app=alertmanager -o wide
            kubectl -n monitoring describe pod -l app=alertmanager
            kubectl -n monitoring logs -l app=alertmanager --tail=100 || true
            
            if [ $i -lt 5 ]; then
              echo "üîÑ Red√©marrage du d√©ploiement..."
              kubectl -n monitoring rollout restart deployment alertmanager
              sleep 20
            fi
          done
          
          echo "‚ùå √âchec apr√®s 5 tentatives"
          exit 1

      - name: ‚éà Installer Prometheus+Grafana avec Helm
        timeout-minutes: 25
        run: |
          # Nettoyer les op√©rations Helm bloqu√©es si n√©cessaire
          helm ls -n monitoring
          helm rollback -n monitoring monitoring 0 || true
          
          # Ajouter le repo avec timeout augment√©
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts --force-update
          
          # Mise √† jour des repos avec plus de temps et de retries
          for i in {1..3}; do
            if helm repo update; then
              echo "‚úÖ Mise √† jour des d√©p√¥ts Helm r√©ussie"
              break
            else
              echo "‚ö†Ô∏è √âchec de la mise √† jour des d√©p√¥ts (tentative $i/3)"
              sleep 10
            fi
          done
          
          # Installation avec v√©rification des op√©rations en cours
          helm upgrade --install monitoring prometheus-community/kube-prometheus-stack \
            --namespace monitoring \
            --create-namespace \
            --wait \
            --timeout 10m \
            --atomic \
            --set grafana.adminUser="${{ secrets.GRAFANA_ADMIN_USER }}" \
            --set grafana.adminPassword="${{ secrets.GRAFANA_ADMIN_PASSWORD }}" \
            --set prometheus.service.type=NodePort \
            --set prometheus.service.nodePort=$PROMETHEUS_PORT \
            --set grafana.service.type=NodePort \
            --set grafana.service.nodePort=$GRAFANA_PORT \
            --set alertmanager.enabled=false \
            --set prometheus.prometheusSpec.alerting.alertmanagers[0].apiVersion="v2" \
            --set prometheus.prometheusSpec.alerting.alertmanagers[0].scheme="http" \
            --set prometheus.prometheusSpec.alerting.alertmanagers[0].staticConfigs[0].targets[0]="alertmanager.monitoring.svc:9093" \
            --set prometheus.prometheusSpec.alerting.alertmanagers[0].pathPrefix="/" \
            --set prometheus.prometheusSpec.alerting.alertmanagers[0].timeout="10s" \
            --set prometheus.prometheusSpec.alertmanagerConfigSelector.matchLabels.release="monitoring" \
            --set prometheus.prometheusSpec.alertmanagerConfigNamespaceSelector.matchLabels.name="monitoring" \
            --set prometheus.prometheusSpec.ruleSelectorNilUsesHelmValues=false \
            --set prometheus.prometheusSpec.ruleSelector.matchLabels.release="monitoring" \
            --set prometheus.prometheusSpec.ruleSelector.matchLabels.app="kube-prometheus-stack" \
            --set prometheus.prometheusSpec.ruleNamespaceSelector.matchLabels.name="monitoring" \
            --set prometheusOperator.prometheusConfigReloader.enabled=true

      - name: ‚è≥ Attendre que Prometheus soit pr√™t
        run: |
          for i in {1..30}; do
            if kubectl -n monitoring get pods -l app.kubernetes.io/name=prometheus -o jsonpath='{.items[0].status.phase}' | grep -q "Running"; then
              echo "‚úÖ Prometheus est en cours d'ex√©cution"
              # Attendre que Prometheus soit vraiment pr√™t
              kubectl -n monitoring wait --for=condition=ready pod -l app.kubernetes.io/name=prometheus --timeout=120s
              sleep 30
              exit 0
            fi
            sleep 10
            echo "‚åõ Tentative $i/30 - En attente du d√©marrage de Prometheus..."
          done
          echo "‚ùå Prometheus n'a pas d√©marr√© apr√®s 5 minutes"
          exit 1

      - name: üö® D√©ployer les r√®gles d'alerte
        run: |
          kubectl apply -f monitoring-config/Prometheus/alert-rules.yml
          # V√©rifier que les r√®gles sont bien cr√©√©es
          kubectl -n monitoring get prometheusrules -o yaml
          sleep 30  # Donner du temps √† Prometheus pour charger les r√®gles

      - name: üîÑ V√©rifier les r√®gles d'alerte
        run: |
          echo "=== V√©rification des r√®gles d√©ploy√©es ==="
          kubectl -n monitoring get prometheusrules -o wide
          
          echo "=== D√©tail des r√®gles YAML ==="
          kubectl -n monitoring get prometheusrules -o yaml
          
          echo "=== Contenu des r√®gles dans Prometheus ==="
          NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
          echo "Acc√®s √† Prometheus via: http://$NODE_IP:$PROMETHEUS_PORT"
          
          # V√©rification des r√®gles via l'API Prometheus
          curl -s "http://$NODE_IP:$PROMETHEUS_PORT/api/v1/rules" | jq .
          
          # V√©rification sp√©cifique des alertes
          echo "=== R√®gles d'alerte actives ==="
          curl -s "http://$NODE_IP:$PROMETHEUS_PORT/api/v1/alerts" | jq '.data.alerts[] | {state, name}'
          
          # V√©rifier les logs de Prometheus pour les erreurs de r√®gles
          echo "=== Logs Prometheus (erreurs r√®gles) ==="
          kubectl -n monitoring logs -l app.kubernetes.io/name=prometheus | grep -i -E 'rule|alert' | tail -20

      - name: üì© Tester les notifications
        run: |
          # R√©cup√©rer l'IP du node
          NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
          ALERTMANAGER_URL="http://$NODE_IP:$ALERTMANAGER_PORT"
          
          echo "=== Envoi d'alerte test √† $ALERTMANAGER_URL ==="
          curl -v -X POST "$ALERTMANAGER_URL/api/v2/alerts" \
            -H "Content-Type: application/json" \
            -d '[{
                "labels": { 
                  "alertname": "TestAlert",
                  "severity": "warning",
                  "namespace": "monitoring"
                },
                "annotations": {
                  "summary": "Alerte de test",
                  "description": "Ceci est une alerte de test envoy√©e depuis GitHub Actions"
                }
              }]'
          
          echo "=== V√©rification des alertes actives ==="
          sleep 5  # Donner le temps √† AlertManager de traiter l'alerte
          curl -s "$ALERTMANAGER_URL/api/v2/alerts" | jq .
          
          echo "=== V√©rification dans Prometheus ==="
          PROMETHEUS_URL="http://$NODE_IP:$PROMETHEUS_PORT"
          curl -s "$PROMETHEUS_URL/api/v1/alerts" | jq '.data.alerts[] | select(.labels.alertname == "TestAlert")'
      - name: üåê Afficher les acc√®s
        run: |
          NODE_IP=$(kubectl get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}')
          cat <<EOF
          ================================================
          üîî MONITORING STACK OP√âRATIONNEL
          ================================================
          
          üìä Acc√®s aux interfaces :
          - Prometheus:    http://$NODE_IP:$PROMETHEUS_PORT
          - AlertManager:  http://$NODE_IP:$ALERTMANAGER_PORT
          - Grafana:       http://$NODE_IP:$GRAFANA_PORT
          
          üîë Identifiants Grafana:
          - Utilisateur: ${{ secrets.GRAFANA_ADMIN_USER }}
          - Mot de passe: ${{ secrets.GRAFANA_ADMIN_PASSWORD }}
          
          üìß Notifications envoy√©es √†: ${{ secrets.SMTP_USER }}
          ================================================
          EOF